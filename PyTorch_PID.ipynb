{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import root_numpy as rnp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tnrange\n",
    "\n",
    "path = 'path/to/your/data' \n",
    "\n",
    "branchs = ['Zmomentum','Momentum','Theta','Position',\\\n",
    "                'MvdDEDX','MvdHits','SttMeanDEDX',\\\n",
    "                'SttHits','GemHits','TofStopTime','TofM2','TofTrackLength',\\\n",
    "                'TofQuality','DrcThetaC','DrcQuality',\\\n",
    "                'DiscThetaC','DiscQuality',\\\n",
    "                'RichThetaC','RichQuality',\\\n",
    "                'EmcRawEnergy','EmcCalEnergy','EmcQuality',\\\n",
    "                'EmcNumberOfCrystals','EmcNumberOfBumps','EmcModule','EmcZ20',\\\n",
    "                'EmcZ53','EmcLat','EmcE1','EmcE9','EmcE25',\\\n",
    "                'MuoNumberOfLayers',\\\n",
    "                'MuoHits','DegreesOfFreedom','FitStatus','ChiSquared']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 50 # change this to -1 to get all data avaliable\n",
    "def get_box_data(path=path, branch_names=branchs):\n",
    "    # TO DO path must be adjust to where the data is\n",
    "    file1 = rnp.root2array(path+'/box/box_500k_electrons.root', 't1', branch_names)\n",
    "    file1 = rnp.rec2array(file1)\n",
    "    file1 = file1[0:amount,:]\n",
    "    \n",
    "    file2 = rnp.root2array(path+'/box/box_500k_pions.root', 't1', branch_names)\n",
    "    file2 = rnp.rec2array(file2)\n",
    "    file2 = file2[0:amount,:]\n",
    "    \n",
    "    file3 = rnp.root2array(path+'/box/box_500k_muons.root', 't1', branch_names)\n",
    "    file3 = rnp.rec2array(file3)\n",
    "    file3 = file3[0:amount,:]\n",
    "\n",
    "    file4 = rnp.root2array(path+'/box/box_500k_kaons.root', 't1', branch_names)\n",
    "    file4 = rnp.rec2array(file4)\n",
    "    file4 = file4[0:amount,:]\n",
    "    \n",
    "    file5 = rnp.root2array(path+'/box/box_500k_protons.root', 't1', branch_names)\n",
    "    file5 = rnp.rec2array(file5)\n",
    "    file5 = file5[0:amount,:]\n",
    "    \n",
    "    file6 = rnp.root2array(path+'/box/box_500k_anti_electrons.root', 't1', branch_names)\n",
    "    file6 = rnp.rec2array(file6)\n",
    "    file6 = file6[0:amount,:]\n",
    "    \n",
    "    file7 = rnp.root2array(path+'/box/box_500k_anti_pions.root', 't1', branch_names)\n",
    "    file7 = rnp.rec2array(file7)      \n",
    "    file7 = file7[0:amount,:]\n",
    "    \n",
    "    file8 = rnp.root2array(path+'/box/box_500k_anti_muons.root', 't1', branch_names)\n",
    "    file8 = rnp.rec2array(file8)\n",
    "    file8 = file8[0:amount,:]\n",
    "    \n",
    "    file9 = rnp.root2array(path+'/box/box_500k_anti_kaons.root', 't1', branch_names)\n",
    "    file9 = rnp.rec2array(file9)     \n",
    "    file9 = file9[0:amount,:]\n",
    "    \n",
    "    file10 = rnp.root2array(path+'/box/box_500k_anti_protons.root', 't1', branch_names)\n",
    "    file10 = rnp.rec2array(file10)\n",
    "    file10 = file10[0:amount,:]\n",
    "    \n",
    "    X = np.concatenate((file1, file2, file3, file4, file5, file6, file7, file8, file9, file10))\n",
    "    y = np.concatenate(( np.zeros(file1.shape[0]), np.zeros(file6.shape[0]),\\\n",
    "                         np.ones(file2.shape[0]), np.ones(file7.shape[0]),\\\n",
    "                         2*np.ones(file3.shape[0]), 2*np.ones(file8.shape[0]),\\\n",
    "                         3*np.ones(file4.shape[0]), 3*np.ones(file9.shape[0]),\\\n",
    "                         4*np.ones(file5.shape[0]), 4*np.ones(file10.shape[0]) )) \n",
    "    \n",
    "    df_ = pd.DataFrame(np.hstack((X, y.reshape(y.shape[0], -1))),columns=branch_names+['temp'])\n",
    "    df_['E/p'] = df_.loc[:,'EmcCalEnergy']/df_.loc[:,'Momentum']\n",
    "    df_['labels'] = df_.loc[:,'temp']\n",
    "    df_ = df_.drop(['temp'], axis=1)\n",
    "    df_ = df_.dropna()\n",
    "    return df_\n",
    "\n",
    "def get_evt_data(path=path, branch_names=branchs):\n",
    "    file1 = rnp.root2array(path+'/evt/evt_500k_electrons.root', 't1', branch_names)\n",
    "    file1 = rnp.rec2array(file1)\n",
    "    file1 = file1[0:amount*2,:]\n",
    "    \n",
    "    file2 = rnp.root2array(path+'/evt/evt_500k_pions.root', 't1', branch_names)\n",
    "    file2 = rnp.rec2array(file2)\n",
    "    file2 = file2[0:amount*2,:]\n",
    "    \n",
    "    file3 = rnp.root2array(path+'/evt/evt_500k_muons.root', 't1', branch_names)\n",
    "    file3 = rnp.rec2array(file3)\n",
    "    file3 = file3[0:amount*2,:]\n",
    "\n",
    "    file4 = rnp.root2array(path+'/evt/evt_500k_kaons.root', 't1', branch_names)\n",
    "    file4 = rnp.rec2array(file4)\n",
    "    file4 = file4[0:amount*2,:]\n",
    "    \n",
    "    file5 = rnp.root2array(path+'/evt/evt_500k_protons.root', 't1', branch_names)\n",
    "    file5 = rnp.rec2array(file5)\n",
    "    file5 = file5[0:amount*2,:]\n",
    "    \n",
    "    X = np.concatenate((file1, file2, file3, file4, file5))\n",
    "    y = np.concatenate(( np.zeros(file1.shape[0]),\\\n",
    "                         np.ones(file2.shape[0]), \\\n",
    "                         2*np.ones(file3.shape[0]),\\\n",
    "                         3*np.ones(file4.shape[0]),\\\n",
    "                         4*np.ones(file5.shape[0]) ))\n",
    "    \n",
    "    df_ = pd.DataFrame(np.hstack((X, y.reshape(y.shape[0], -1))),columns=branch_names+['temp'])\n",
    "    df_['E/p'] = df_.loc[:,'EmcCalEnergy']/df_.loc[:,'Momentum']\n",
    "    df_['labels'] = df_.loc[:,'temp']\n",
    "    df_ = df_.drop(['temp'], axis=1)\n",
    "    df_ = df_.dropna()\n",
    "    return df_\n",
    "\n",
    "def get_dpm_data(path=path, branch_names=branchs):\n",
    "    file1 = rnp.root2array(path+'/dpm/dpmbkg_1M_electrons.root', 't1', branch_names)\n",
    "    file1 = rnp.rec2array(file1)\n",
    "    file1 = file1[0:17000,:]\n",
    "    \n",
    "    file2 = rnp.root2array(path+'/dpm/dpmbkg_1M_pions.root', 't1', branch_names)\n",
    "    file2 = rnp.rec2array(file2)\n",
    "    file2 = file2[0:17000,:]\n",
    "    \n",
    "    file3 = rnp.root2array(path+'/dpm/dpmbkg_1M_muons.root', 't1', branch_names)\n",
    "    file3 = rnp.rec2array(file3)\n",
    "    file3 = file3[0:17000,:]\n",
    "\n",
    "    file4 = rnp.root2array(path+'/dpm/dpmbkg_1M_kaons.root', 't1', branch_names)\n",
    "    file4 = rnp.rec2array(file4)\n",
    "    file4 = file4[0:17000,:]\n",
    "    \n",
    "    file5 = rnp.root2array(path+'/dpm/dpmbkg_1M_protons.root', 't1', branch_names)\n",
    "    file5 = rnp.rec2array(file5)\n",
    "    file5 = file5[0:15000,:]\n",
    "    \n",
    "    X = np.concatenate((file1, file2, file3, file4, file5))\n",
    "    y = np.concatenate(( np.zeros(file1.shape[0]),\\\n",
    "                         np.ones(file2.shape[0]), \\\n",
    "                         2*np.ones(file3.shape[0]),\\\n",
    "                         3*np.ones(file4.shape[0]),\\\n",
    "                         4*np.ones(file5.shape[0]) ))\n",
    "    \n",
    "    df_ = pd.DataFrame(np.hstack((X, y.reshape(y.shape[0], -1))),columns=branch_names+['temp'])\n",
    "    df_['E/p'] = df_.loc[:,'EmcCalEnergy']/df_.loc[:,'Momentum']\n",
    "    df_['labels'] = df_.loc[:,'temp']\n",
    "    df_ = df_.drop(['temp'], axis=1)\n",
    "    df_ = df_.dropna()\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.1 s, sys: 1.32 s, total: 26.4 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "Box = get_box_data()\n",
    "Evt = get_evt_data()\n",
    "Dpm = get_dpm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "class DataGenerator(Dataset):\n",
    "    '''Create PndFts HitPair Dataset'''\n",
    "\n",
    "    def __init__(self, box=None, evt=None, dpm=None, transform=None):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.box         = box\n",
    "        self.evt         = evt\n",
    "        self.dpm         = dpm      \n",
    "        self.transform   = transform\n",
    "        \n",
    "        self.__X, self.__y = self.__CombineAll()\n",
    "        \n",
    "    def __CombineAll(self):\n",
    "        if ((self.box is None) or (self.evt is None) or (self.dpm is None)):\n",
    "            raise ValueError('Datasets must be provided: box=? evt=? dpm=?')\n",
    "        box_data = np.array(self.box.iloc[:,0:-1])\n",
    "        evt_data = np.array(self.evt.iloc[:,0:-1])\n",
    "        dpm_data = np.array(self.dpm.iloc[:,0:-1])\n",
    "\n",
    "        box_labels  = np.array(self.box.iloc[:,-1])\n",
    "        evt_labels  = np.array(self.evt.iloc[:,-1])\n",
    "        dpm_labels  = np.array(self.dpm.iloc[:,-1])\n",
    "\n",
    "        train_data_x = np.vstack([dpm_data, evt_data, box_data])\n",
    "        train_data_y = np.hstack([dpm_labels,evt_labels,box_labels])\n",
    "        \n",
    "        train_data_x = torch.from_numpy(train_data_x)\n",
    "        train_data_y = torch.from_numpy(train_data_y)\n",
    "        \n",
    "        train_data_x = train_data_x.to(device=device, dtype=torch.float32)\n",
    "        train_data_y = train_data_y.to(device=device, dtype=torch.long)\n",
    "        \n",
    "        return train_data_x, train_data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.__X[idx], self.__y[idx]\n",
    "    \n",
    "    def getX(self):\n",
    "        return self.__X\n",
    "    \n",
    "    def gety(self):\n",
    "        return self.__y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataGenerator(box=Box, evt=Evt, dpm=Dpm, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=37, out_features=500, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.25, inplace=False)\n",
      "  (3): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): Linear(in_features=500, out_features=400, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Dropout(p=0.25, inplace=False)\n",
      "  (7): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (11): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (12): ReLU()\n",
      "  (13): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (15): ReLU()\n",
      "  (16): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (17): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (18): ReLU()\n",
      "  (19): Linear(in_features=50, out_features=5, bias=True)\n",
      "  (20): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#network architecture\n",
    "net = nn.Sequential(nn.Linear(37, 500),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.BatchNorm1d(500),\n",
    "                      nn.Linear(500, 400),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.BatchNorm1d(400),                    \n",
    "                      nn.Linear(400, 300),\n",
    "                      nn.ReLU(),\n",
    "                      #nn.Dropout(p=0.5),\n",
    "                      nn.BatchNorm1d(300),\n",
    "                      nn.Linear(300, 200),\n",
    "                      nn.ReLU(),\n",
    "                      #nn.Dropout(p=0.5),\n",
    "                      nn.BatchNorm1d(200),\n",
    "                      nn.Linear(200, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(100),\n",
    "                      nn.Linear(100, 50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(50, 5),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a loss and optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/21], Loss: 1.6217\n",
      "Epoch [1/3], Step [2/21], Loss: 1.5748\n",
      "Epoch [1/3], Step [3/21], Loss: 1.5503\n",
      "Epoch [1/3], Step [4/21], Loss: 1.5612\n",
      "Epoch [1/3], Step [5/21], Loss: 1.5353\n",
      "Epoch [1/3], Step [6/21], Loss: 1.5160\n",
      "Epoch [1/3], Step [7/21], Loss: 1.5134\n",
      "Epoch [1/3], Step [8/21], Loss: 1.5433\n",
      "Epoch [1/3], Step [9/21], Loss: 1.4868\n",
      "Epoch [1/3], Step [10/21], Loss: 1.5006\n",
      "Epoch [1/3], Step [11/21], Loss: 1.4837\n",
      "Epoch [1/3], Step [12/21], Loss: 1.4807\n",
      "Epoch [1/3], Step [13/21], Loss: 1.4675\n",
      "Epoch [1/3], Step [14/21], Loss: 1.4632\n",
      "Epoch [1/3], Step [15/21], Loss: 1.4529\n",
      "Epoch [1/3], Step [16/21], Loss: 1.5003\n",
      "Epoch [1/3], Step [17/21], Loss: 1.4553\n",
      "Epoch [1/3], Step [18/21], Loss: 1.4520\n",
      "Epoch [1/3], Step [19/21], Loss: 1.4413\n",
      "Epoch [1/3], Step [20/21], Loss: 1.4273\n",
      "Epoch [1/3], Step [21/21], Loss: 1.4162\n",
      "Epoch [2/3], Step [1/21], Loss: 1.4175\n",
      "Epoch [2/3], Step [2/21], Loss: 1.4557\n",
      "Epoch [2/3], Step [3/21], Loss: 1.4109\n",
      "Epoch [2/3], Step [4/21], Loss: 1.4118\n",
      "Epoch [2/3], Step [5/21], Loss: 1.4246\n",
      "Epoch [2/3], Step [6/21], Loss: 1.4194\n",
      "Epoch [2/3], Step [7/21], Loss: 1.4131\n",
      "Epoch [2/3], Step [8/21], Loss: 1.4077\n",
      "Epoch [2/3], Step [9/21], Loss: 1.3995\n",
      "Epoch [2/3], Step [10/21], Loss: 1.4765\n",
      "Epoch [2/3], Step [11/21], Loss: 1.4061\n",
      "Epoch [2/3], Step [12/21], Loss: 1.3969\n",
      "Epoch [2/3], Step [13/21], Loss: 1.3889\n",
      "Epoch [2/3], Step [14/21], Loss: 1.3863\n",
      "Epoch [2/3], Step [15/21], Loss: 1.4081\n",
      "Epoch [2/3], Step [16/21], Loss: 1.4117\n",
      "Epoch [2/3], Step [17/21], Loss: 1.3906\n",
      "Epoch [2/3], Step [18/21], Loss: 1.4286\n",
      "Epoch [2/3], Step [19/21], Loss: 1.3962\n",
      "Epoch [2/3], Step [20/21], Loss: 1.3658\n",
      "Epoch [2/3], Step [21/21], Loss: 1.4040\n",
      "Epoch [3/3], Step [1/21], Loss: 1.3708\n",
      "Epoch [3/3], Step [2/21], Loss: 1.3804\n",
      "Epoch [3/3], Step [3/21], Loss: 1.3788\n",
      "Epoch [3/3], Step [4/21], Loss: 1.3744\n",
      "Epoch [3/3], Step [5/21], Loss: 1.3861\n",
      "Epoch [3/3], Step [6/21], Loss: 1.3745\n",
      "Epoch [3/3], Step [7/21], Loss: 1.3649\n",
      "Epoch [3/3], Step [8/21], Loss: 1.3632\n",
      "Epoch [3/3], Step [9/21], Loss: 1.3813\n",
      "Epoch [3/3], Step [10/21], Loss: 1.3853\n",
      "Epoch [3/3], Step [11/21], Loss: 1.3574\n",
      "Epoch [3/3], Step [12/21], Loss: 1.4321\n",
      "Epoch [3/3], Step [13/21], Loss: 1.3532\n",
      "Epoch [3/3], Step [14/21], Loss: 1.3740\n",
      "Epoch [3/3], Step [15/21], Loss: 1.3558\n",
      "Epoch [3/3], Step [16/21], Loss: 1.3518\n",
      "Epoch [3/3], Step [17/21], Loss: 1.3761\n",
      "Epoch [3/3], Step [18/21], Loss: 1.3348\n",
      "Epoch [3/3], Step [19/21], Loss: 1.3386\n",
      "Epoch [3/3], Step [20/21], Loss: 1.3432\n",
      "Epoch [3/3], Step [21/21], Loss: 1.3274\n"
     ]
    }
   ],
   "source": [
    "# run the main training loop\n",
    "total_step = len(dataloader)\n",
    "kEpochs = 3\n",
    "for epoch in range(kEpochs):\n",
    "    for i_batch, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if (i+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\\\n",
    "                   .format(epoch+1, kEpochs, i_batch+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat  tensor([3, 2, 1,  ..., 2, 0, 3])\n",
      "y     tensor([4, 2, 0,  ..., 0, 3, 1])\n",
      "yhat  tensor([3, 3, 2,  ..., 2, 2, 2])\n",
      "y     tensor([3, 1, 1,  ..., 0, 2, 2])\n",
      "yhat  tensor([0, 4, 2,  ..., 1, 3, 3])\n",
      "y     tensor([1, 0, 3,  ..., 3, 1, 1])\n",
      "yhat  tensor([2, 1, 0,  ..., 3, 3, 3])\n",
      "y     tensor([2, 0, 1,  ..., 2, 1, 2])\n",
      "yhat  tensor([3, 1, 2,  ..., 2, 4, 2])\n",
      "y     tensor([4, 1, 0,  ..., 2, 4, 0])\n",
      "yhat  tensor([2, 0, 3,  ..., 4, 3, 2])\n",
      "y     tensor([2, 1, 1,  ..., 4, 4, 2])\n",
      "yhat  tensor([3, 4, 3,  ..., 2, 0, 2])\n",
      "y     tensor([1, 3, 4,  ..., 3, 1, 2])\n",
      "yhat  tensor([3, 2, 1,  ..., 1, 3, 0])\n",
      "y     tensor([0, 1, 1,  ..., 2, 1, 0])\n",
      "yhat  tensor([4, 2, 0,  ..., 2, 4, 2])\n",
      "y     tensor([3, 0, 0,  ..., 0, 4, 2])\n",
      "yhat  tensor([0, 2, 0,  ..., 2, 4, 0])\n",
      "y     tensor([0, 2, 0,  ..., 1, 3, 0])\n",
      "yhat  tensor([3, 0, 2,  ..., 3, 0, 2])\n",
      "y     tensor([1, 0, 3,  ..., 3, 0, 2])\n",
      "yhat  tensor([4, 0, 3,  ..., 1, 3, 0])\n",
      "y     tensor([3, 0, 0,  ..., 3, 1, 0])\n",
      "yhat  tensor([3, 3, 0,  ..., 2, 4, 4])\n",
      "y     tensor([3, 3, 0,  ..., 0, 4, 4])\n",
      "yhat  tensor([2, 3, 2,  ..., 2, 4, 3])\n",
      "y     tensor([0, 3, 2,  ..., 2, 4, 2])\n",
      "yhat  tensor([0, 3, 2,  ..., 3, 4, 2])\n",
      "y     tensor([0, 4, 1,  ..., 0, 4, 2])\n",
      "yhat  tensor([1, 0, 0,  ..., 2, 2, 2])\n",
      "y     tensor([3, 0, 2,  ..., 2, 1, 1])\n",
      "yhat  tensor([3, 3, 2,  ..., 3, 2, 3])\n",
      "y     tensor([1, 3, 0,  ..., 3, 1, 0])\n",
      "yhat  tensor([2, 3, 4,  ..., 1, 0, 0])\n",
      "y     tensor([4, 3, 4,  ..., 3, 0, 3])\n",
      "yhat  tensor([0, 0, 3,  ..., 2, 3, 4])\n",
      "y     tensor([4, 0, 4,  ..., 3, 1, 4])\n",
      "yhat  tensor([0, 1, 0,  ..., 1, 0, 3])\n",
      "y     tensor([0, 4, 2,  ..., 2, 2, 1])\n",
      "yhat  tensor([0, 3, 2,  ..., 2, 3, 2])\n",
      "y     tensor([2, 4, 0,  ..., 2, 4, 1])\n",
      "Accuracy: 41.75 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i_batch, (inputs, labels) in enumerate(dataloader):\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy: {:.2f} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
